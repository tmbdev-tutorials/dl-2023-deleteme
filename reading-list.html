<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>Bibliography</title>
</head>
<body>
<div class="csl-bib-body" style="line-height: 1.35; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Almahairi, Amjad, Sai Rajeshwar, Alessandro Sordoni, Philip Bachman, and Aaron Courville. “Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data.” In <i>Proceedings of the 35th International Conference on Machine Learning</i>, 195–204. PMLR, 2018. <a href="https://proceedings.mlr.press/v80/almahairi18a.html">https://proceedings.mlr.press/v80/almahairi18a.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Augmented%20CycleGAN%3A%20Learning%20Many-to-Many%20Mappings%20from%20Unpaired%20Data&amp;rft.btitle=Proceedings%20of%20the%2035th%20International%20Conference%20on%20Machine%20Learning&amp;rft.publisher=PMLR&amp;rft.aufirst=Amjad&amp;rft.aulast=Almahairi&amp;rft.au=Amjad%20Almahairi&amp;rft.au=Sai%20Rajeshwar&amp;rft.au=Alessandro%20Sordoni&amp;rft.au=Philip%20Bachman&amp;rft.au=Aaron%20Courville&amp;rft.date=2018-07-03&amp;rft.pages=195-204&amp;rft.spage=195&amp;rft.epage=204&amp;rft.language=en"></span>
  <div class="csl-entry">Arjovsky, Martin, Soumith Chintala, and Léon Bottou. “Wasserstein GAN.” arXiv, December 6, 2017. <a href="https://doi.org/10.48550/arXiv.1701.07875">https://doi.org/10.48550/arXiv.1701.07875</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1701.07875&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Wasserstein%20GAN&amp;rft.description=We%20introduce%20a%20new%20algorithm%20named%20WGAN%2C%20an%20alternative%20to%20traditional%20GAN%20training.%20In%20this%20new%20model%2C%20we%20show%20that%20we%20can%20improve%20the%20stability%20of%20learning%2C%20get%20rid%20of%20problems%20like%20mode%20collapse%2C%20and%20provide%20meaningful%20learning%20curves%20useful%20for%20debugging%20and%20hyperparameter%20searches.%20Furthermore%2C%20we%20show%20that%20the%20corresponding%20optimization%20problem%20is%20sound%2C%20and%20provide%20extensive%20theoretical%20work%20highlighting%20the%20deep%20connections%20to%20other%20distances%20between%20distributions.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1701.07875&amp;rft.aufirst=Martin&amp;rft.aulast=Arjovsky&amp;rft.au=Martin%20Arjovsky&amp;rft.au=Soumith%20Chintala&amp;rft.au=L%C3%A9on%20Bottou&amp;rft.date=2017-12-06"></span>
  <div class="csl-entry">Bao, Hangbo, Li Dong, Songhao Piao, and Furu Wei. “BEiT: BERT Pre-Training of Image Transformers.” arXiv, September 3, 2022. <a href="https://doi.org/10.48550/arXiv.2106.08254">https://doi.org/10.48550/arXiv.2106.08254</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2106.08254&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=BEiT%3A%20BERT%20Pre-Training%20of%20Image%20Transformers&amp;rft.description=We%20introduce%20a%20self-supervised%20vision%20representation%20model%20BEiT%2C%20which%20stands%20for%20Bidirectional%20Encoder%20representation%20from%20Image%20Transformers.%20Following%20BERT%20developed%20in%20the%20natural%20language%20processing%20area%2C%20we%20propose%20a%20masked%20image%20modeling%20task%20to%20pretrain%20vision%20Transformers.%20Specifically%2C%20each%20image%20has%20two%20views%20in%20our%20pre-training%2C%20i.e%2C%20image%20patches%20(such%20as%2016x16%20pixels)%2C%20and%20visual%20tokens%20(i.e.%2C%20discrete%20tokens).%20We%20first%20%22tokenize%22%20the%20original%20image%20into%20visual%20tokens.%20Then%20we%20randomly%20mask%20some%20image%20patches%20and%20fed%20them%20into%20the%20backbone%20Transformer.%20The%20pre-training%20objective%20is%20to%20recover%20the%20original%20visual%20tokens%20based%20on%20the%20corrupted%20image%20patches.%20After%20pre-training%20BEiT%2C%20we%20directly%20fine-tune%20the%20model%20parameters%20on%20downstream%20tasks%20by%20appending%20task%20layers%20upon%20the%20pretrained%20encoder.%20Experimental%20results%20on%20image%20classification%20and%20semantic%20segmentation%20show%20that%20our%20model%20achieves%20competitive%20results%20with%20previous%20pre-training%20methods.%20For%20example%2C%20base-size%20BEiT%20achieves%2083.2%25%20top-1%20accuracy%20on%20ImageNet-1K%2C%20significantly%20outperforming%20from-scratch%20DeiT%20training%20(81.8%25)%20with%20the%20same%20setup.%20Moreover%2C%20large-size%20BEiT%20obtains%2086.3%25%20only%20using%20ImageNet-1K%2C%20even%20outperforming%20ViT-L%20with%20supervised%20pre-training%20on%20ImageNet-22K%20(85.2%25).%20The%20code%20and%20pretrained%20models%20are%20available%20at%20https%3A%2F%2Faka.ms%2Fbeit.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2106.08254&amp;rft.aufirst=Hangbo&amp;rft.aulast=Bao&amp;rft.au=Hangbo%20Bao&amp;rft.au=Li%20Dong&amp;rft.au=Songhao%20Piao&amp;rft.au=Furu%20Wei&amp;rft.date=2022-09-03"></span>
  <div class="csl-entry">Bell, Anthony J., and Terrence J. Sejnowski. “An Information-Maximization Approach to Blind Separation and Blind Deconvolution.” <i>Neural Computation</i> 7 (1995): 1129–59.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=An%20Information-Maximization%20Approach%20to%20Blind%20Separation%20and%20Blind%20Deconvolution&amp;rft.jtitle=Neural%20Computation&amp;rft.volume=7&amp;rft.aufirst=Anthony%20J.&amp;rft.aulast=Bell&amp;rft.au=Anthony%20J.%20Bell&amp;rft.au=Terrence%20J.%20Sejnowski&amp;rft.date=1995&amp;rft.pages=1129%E2%80%931159&amp;rft.spage=1129&amp;rft.epage=1159"></span>
  <div class="csl-entry">BELL, ANTHONY J., and TERRENCE J. SEJNOWSKI. “The ‘Independent Components’ of Natural Scenes Are Edge Filters.” <i>Vision Research</i> 37, no. 23 (December 1997): 3327–38.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Apmid%2F9425547&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The%20%E2%80%9CIndependent%20Components%E2%80%9D%20of%20Natural%20Scenes%20are%20Edge%20Filters&amp;rft.jtitle=Vision%20research&amp;rft.stitle=Vision%20Res&amp;rft.volume=37&amp;rft.issue=23&amp;rft.aufirst=ANTHONY%20J.&amp;rft.aulast=BELL&amp;rft.au=ANTHONY%20J.%20BELL&amp;rft.au=TERRENCE%20J.%20SEJNOWSKI&amp;rft.date=1997-12&amp;rft.pages=3327-3338&amp;rft.spage=3327&amp;rft.epage=3338&amp;rft.issn=0042-6989"></span>
  <div class="csl-entry">Brock, Andrew, Jeff Donahue, and Karen Simonyan. “Large Scale GAN Training for High Fidelity Natural Image Synthesis.” arXiv, February 25, 2019. <a href="https://doi.org/10.48550/arXiv.1809.11096">https://doi.org/10.48550/arXiv.1809.11096</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1809.11096&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Large%20Scale%20GAN%20Training%20for%20High%20Fidelity%20Natural%20Image%20Synthesis&amp;rft.description=Despite%20recent%20progress%20in%20generative%20image%20modeling%2C%20successfully%20generating%20high-resolution%2C%20diverse%20samples%20from%20complex%20datasets%20such%20as%20ImageNet%20remains%20an%20elusive%20goal.%20To%20this%20end%2C%20we%20train%20Generative%20Adversarial%20Networks%20at%20the%20largest%20scale%20yet%20attempted%2C%20and%20study%20the%20instabilities%20specific%20to%20such%20scale.%20We%20find%20that%20applying%20orthogonal%20regularization%20to%20the%20generator%20renders%20it%20amenable%20to%20a%20simple%20%22truncation%20trick%2C%22%20allowing%20fine%20control%20over%20the%20trade-off%20between%20sample%20fidelity%20and%20variety%20by%20reducing%20the%20variance%20of%20the%20Generator's%20input.%20Our%20modifications%20lead%20to%20models%20which%20set%20the%20new%20state%20of%20the%20art%20in%20class-conditional%20image%20synthesis.%20When%20trained%20on%20ImageNet%20at%20128x128%20resolution%2C%20our%20models%20(BigGANs)%20achieve%20an%20Inception%20Score%20(IS)%20of%20166.5%20and%20Frechet%20Inception%20Distance%20(FID)%20of%207.4%2C%20improving%20over%20the%20previous%20best%20IS%20of%2052.52%20and%20FID%20of%2018.6.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1809.11096&amp;rft.aufirst=Andrew&amp;rft.aulast=Brock&amp;rft.au=Andrew%20Brock&amp;rft.au=Jeff%20Donahue&amp;rft.au=Karen%20Simonyan&amp;rft.date=2019-02-25"></span>
  <div class="csl-entry">Bromley, Jane, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. “Signature Verification Using a ‘Siamese’ Time Delay Neural Network,” n.d., 8.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Signature%20Verification%20using%20a%20%22Siamese%22%20Time%20Delay%20Neural%20Network&amp;rft.aufirst=Jane&amp;rft.aulast=Bromley&amp;rft.au=Jane%20Bromley&amp;rft.au=Isabelle%20Guyon&amp;rft.au=Yann%20LeCun&amp;rft.au=Eduard%20S%C3%A4ckinger&amp;rft.au=Roopak%20Shah&amp;rft.pages=8&amp;rft.language=en"></span>
  <div class="csl-entry">Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. “Language Models Are Few-Shot Learners.” arXiv, July 22, 2020. <a href="https://doi.org/10.48550/arXiv.2005.14165">https://doi.org/10.48550/arXiv.2005.14165</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2005.14165&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Language%20Models%20are%20Few-Shot%20Learners&amp;rft.description=Recent%20work%20has%20demonstrated%20substantial%20gains%20on%20many%20NLP%20tasks%20and%20benchmarks%20by%20pre-training%20on%20a%20large%20corpus%20of%20text%20followed%20by%20fine-tuning%20on%20a%20specific%20task.%20While%20typically%20task-agnostic%20in%20architecture%2C%20this%20method%20still%20requires%20task-specific%20fine-tuning%20datasets%20of%20thousands%20or%20tens%20of%20thousands%20of%20examples.%20By%20contrast%2C%20humans%20can%20generally%20perform%20a%20new%20language%20task%20from%20only%20a%20few%20examples%20or%20from%20simple%20instructions%20-%20something%20which%20current%20NLP%20systems%20still%20largely%20struggle%20to%20do.%20Here%20we%20show%20that%20scaling%20up%20language%20models%20greatly%20improves%20task-agnostic%2C%20few-shot%20performance%2C%20sometimes%20even%20reaching%20competitiveness%20with%20prior%20state-of-the-art%20fine-tuning%20approaches.%20Specifically%2C%20we%20train%20GPT-3%2C%20an%20autoregressive%20language%20model%20with%20175%20billion%20parameters%2C%2010x%20more%20than%20any%20previous%20non-sparse%20language%20model%2C%20and%20test%20its%20performance%20in%20the%20few-shot%20setting.%20For%20all%20tasks%2C%20GPT-3%20is%20applied%20without%20any%20gradient%20updates%20or%20fine-tuning%2C%20with%20tasks%20and%20few-shot%20demonstrations%20specified%20purely%20via%20text%20interaction%20with%20the%20model.%20GPT-3%20achieves%20strong%20performance%20on%20many%20NLP%20datasets%2C%20including%20translation%2C%20question-answering%2C%20and%20cloze%20tasks%2C%20as%20well%20as%20several%20tasks%20that%20require%20on-the-fly%20reasoning%20or%20domain%20adaptation%2C%20such%20as%20unscrambling%20words%2C%20using%20a%20novel%20word%20in%20a%20sentence%2C%20or%20performing%203-digit%20arithmetic.%20At%20the%20same%20time%2C%20we%20also%20identify%20some%20datasets%20where%20GPT-3's%20few-shot%20learning%20still%20struggles%2C%20as%20well%20as%20some%20datasets%20where%20GPT-3%20faces%20methodological%20issues%20related%20to%20training%20on%20large%20web%20corpora.%20Finally%2C%20we%20find%20that%20GPT-3%20can%20generate%20samples%20of%20news%20articles%20which%20human%20evaluators%20have%20difficulty%20distinguishing%20from%20articles%20written%20by%20humans.%20We%20discuss%20broader%20societal%20impacts%20of%20this%20finding%20and%20of%20GPT-3%20in%20general.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2005.14165&amp;rft.aufirst=Tom%20B.&amp;rft.aulast=Brown&amp;rft.au=Tom%20B.%20Brown&amp;rft.au=Benjamin%20Mann&amp;rft.au=Nick%20Ryder&amp;rft.au=Melanie%20Subbiah&amp;rft.au=Jared%20Kaplan&amp;rft.au=Prafulla%20Dhariwal&amp;rft.au=Arvind%20Neelakantan&amp;rft.au=Pranav%20Shyam&amp;rft.au=Girish%20Sastry&amp;rft.au=Amanda%20Askell&amp;rft.au=Sandhini%20Agarwal&amp;rft.au=Ariel%20Herbert-Voss&amp;rft.au=Gretchen%20Krueger&amp;rft.au=Tom%20Henighan&amp;rft.au=Rewon%20Child&amp;rft.au=Aditya%20Ramesh&amp;rft.au=Daniel%20M.%20Ziegler&amp;rft.au=Jeffrey%20Wu&amp;rft.au=Clemens%20Winter&amp;rft.au=Christopher%20Hesse&amp;rft.au=Mark%20Chen&amp;rft.au=Eric%20Sigler&amp;rft.au=Mateusz%20Litwin&amp;rft.au=Scott%20Gray&amp;rft.au=Benjamin%20Chess&amp;rft.au=Jack%20Clark&amp;rft.au=Christopher%20Berner&amp;rft.au=Sam%20McCandlish&amp;rft.au=Alec%20Radford&amp;rft.au=Ilya%20Sutskever&amp;rft.au=Dario%20Amodei&amp;rft.date=2020-07-22"></span>
  <div class="csl-entry">———. “Language Models Are Few-Shot Learners.” arXiv, July 22, 2020. <a href="https://doi.org/10.48550/arXiv.2005.14165">https://doi.org/10.48550/arXiv.2005.14165</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2005.14165&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Language%20Models%20are%20Few-Shot%20Learners&amp;rft.description=Recent%20work%20has%20demonstrated%20substantial%20gains%20on%20many%20NLP%20tasks%20and%20benchmarks%20by%20pre-training%20on%20a%20large%20corpus%20of%20text%20followed%20by%20fine-tuning%20on%20a%20specific%20task.%20While%20typically%20task-agnostic%20in%20architecture%2C%20this%20method%20still%20requires%20task-specific%20fine-tuning%20datasets%20of%20thousands%20or%20tens%20of%20thousands%20of%20examples.%20By%20contrast%2C%20humans%20can%20generally%20perform%20a%20new%20language%20task%20from%20only%20a%20few%20examples%20or%20from%20simple%20instructions%20-%20something%20which%20current%20NLP%20systems%20still%20largely%20struggle%20to%20do.%20Here%20we%20show%20that%20scaling%20up%20language%20models%20greatly%20improves%20task-agnostic%2C%20few-shot%20performance%2C%20sometimes%20even%20reaching%20competitiveness%20with%20prior%20state-of-the-art%20fine-tuning%20approaches.%20Specifically%2C%20we%20train%20GPT-3%2C%20an%20autoregressive%20language%20model%20with%20175%20billion%20parameters%2C%2010x%20more%20than%20any%20previous%20non-sparse%20language%20model%2C%20and%20test%20its%20performance%20in%20the%20few-shot%20setting.%20For%20all%20tasks%2C%20GPT-3%20is%20applied%20without%20any%20gradient%20updates%20or%20fine-tuning%2C%20with%20tasks%20and%20few-shot%20demonstrations%20specified%20purely%20via%20text%20interaction%20with%20the%20model.%20GPT-3%20achieves%20strong%20performance%20on%20many%20NLP%20datasets%2C%20including%20translation%2C%20question-answering%2C%20and%20cloze%20tasks%2C%20as%20well%20as%20several%20tasks%20that%20require%20on-the-fly%20reasoning%20or%20domain%20adaptation%2C%20such%20as%20unscrambling%20words%2C%20using%20a%20novel%20word%20in%20a%20sentence%2C%20or%20performing%203-digit%20arithmetic.%20At%20the%20same%20time%2C%20we%20also%20identify%20some%20datasets%20where%20GPT-3's%20few-shot%20learning%20still%20struggles%2C%20as%20well%20as%20some%20datasets%20where%20GPT-3%20faces%20methodological%20issues%20related%20to%20training%20on%20large%20web%20corpora.%20Finally%2C%20we%20find%20that%20GPT-3%20can%20generate%20samples%20of%20news%20articles%20which%20human%20evaluators%20have%20difficulty%20distinguishing%20from%20articles%20written%20by%20humans.%20We%20discuss%20broader%20societal%20impacts%20of%20this%20finding%20and%20of%20GPT-3%20in%20general.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2005.14165&amp;rft.aufirst=Tom%20B.&amp;rft.aulast=Brown&amp;rft.au=Tom%20B.%20Brown&amp;rft.au=Benjamin%20Mann&amp;rft.au=Nick%20Ryder&amp;rft.au=Melanie%20Subbiah&amp;rft.au=Jared%20Kaplan&amp;rft.au=Prafulla%20Dhariwal&amp;rft.au=Arvind%20Neelakantan&amp;rft.au=Pranav%20Shyam&amp;rft.au=Girish%20Sastry&amp;rft.au=Amanda%20Askell&amp;rft.au=Sandhini%20Agarwal&amp;rft.au=Ariel%20Herbert-Voss&amp;rft.au=Gretchen%20Krueger&amp;rft.au=Tom%20Henighan&amp;rft.au=Rewon%20Child&amp;rft.au=Aditya%20Ramesh&amp;rft.au=Daniel%20M.%20Ziegler&amp;rft.au=Jeffrey%20Wu&amp;rft.au=Clemens%20Winter&amp;rft.au=Christopher%20Hesse&amp;rft.au=Mark%20Chen&amp;rft.au=Eric%20Sigler&amp;rft.au=Mateusz%20Litwin&amp;rft.au=Scott%20Gray&amp;rft.au=Benjamin%20Chess&amp;rft.au=Jack%20Clark&amp;rft.au=Christopher%20Berner&amp;rft.au=Sam%20McCandlish&amp;rft.au=Alec%20Radford&amp;rft.au=Ilya%20Sutskever&amp;rft.au=Dario%20Amodei&amp;rft.date=2020-07-22"></span>
  <div class="csl-entry">Caron, Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. “Emerging Properties in Self-Supervised Vision Transformers.” arXiv, May 24, 2021. <a href="https://doi.org/10.48550/arXiv.2104.14294">https://doi.org/10.48550/arXiv.2104.14294</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2104.14294&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Emerging%20Properties%20in%20Self-Supervised%20Vision%20Transformers&amp;rft.description=In%20this%20paper%2C%20we%20question%20if%20self-supervised%20learning%20provides%20new%20properties%20to%20Vision%20Transformer%20(ViT)%20that%20stand%20out%20compared%20to%20convolutional%20networks%20(convnets).%20Beyond%20the%20fact%20that%20adapting%20self-supervised%20methods%20to%20this%20architecture%20works%20particularly%20well%2C%20we%20make%20the%20following%20observations%3A%20first%2C%20self-supervised%20ViT%20features%20contain%20explicit%20information%20about%20the%20semantic%20segmentation%20of%20an%20image%2C%20which%20does%20not%20emerge%20as%20clearly%20with%20supervised%20ViTs%2C%20nor%20with%20convnets.%20Second%2C%20these%20features%20are%20also%20excellent%20k-NN%20classifiers%2C%20reaching%2078.3%25%20top-1%20on%20ImageNet%20with%20a%20small%20ViT.%20Our%20study%20also%20underlines%20the%20importance%20of%20momentum%20encoder%2C%20multi-crop%20training%2C%20and%20the%20use%20of%20small%20patches%20with%20ViTs.%20We%20implement%20our%20findings%20into%20a%20simple%20self-supervised%20method%2C%20called%20DINO%2C%20which%20we%20interpret%20as%20a%20form%20of%20self-distillation%20with%20no%20labels.%20We%20show%20the%20synergy%20between%20DINO%20and%20ViTs%20by%20achieving%2080.1%25%20top-1%20on%20ImageNet%20in%20linear%20evaluation%20with%20ViT-Base.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2104.14294&amp;rft.aufirst=Mathilde&amp;rft.aulast=Caron&amp;rft.au=Mathilde%20Caron&amp;rft.au=Hugo%20Touvron&amp;rft.au=Ishan%20Misra&amp;rft.au=Herv%C3%A9%20J%C3%A9gou&amp;rft.au=Julien%20Mairal&amp;rft.au=Piotr%20Bojanowski&amp;rft.au=Armand%20Joulin&amp;rft.date=2021-05-24"></span>
  <div class="csl-entry">Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. “A Simple Framework for Contrastive Learning of Visual Representations.” arXiv, June 30, 2020. <a href="https://doi.org/10.48550/arXiv.2002.05709">https://doi.org/10.48550/arXiv.2002.05709</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2002.05709&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations&amp;rft.description=This%20paper%20presents%20SimCLR%3A%20a%20simple%20framework%20for%20contrastive%20learning%20of%20visual%20representations.%20We%20simplify%20recently%20proposed%20contrastive%20self-supervised%20learning%20algorithms%20without%20requiring%20specialized%20architectures%20or%20a%20memory%20bank.%20In%20order%20to%20understand%20what%20enables%20the%20contrastive%20prediction%20tasks%20to%20learn%20useful%20representations%2C%20we%20systematically%20study%20the%20major%20components%20of%20our%20framework.%20We%20show%20that%20(1)%20composition%20of%20data%20augmentations%20plays%20a%20critical%20role%20in%20defining%20effective%20predictive%20tasks%2C%20(2)%20introducing%20a%20learnable%20nonlinear%20transformation%20between%20the%20representation%20and%20the%20contrastive%20loss%20substantially%20improves%20the%20quality%20of%20the%20learned%20representations%2C%20and%20(3)%20contrastive%20learning%20benefits%20from%20larger%20batch%20sizes%20and%20more%20training%20steps%20compared%20to%20supervised%20learning.%20By%20combining%20these%20findings%2C%20we%20are%20able%20to%20considerably%20outperform%20previous%20methods%20for%20self-supervised%20and%20semi-supervised%20learning%20on%20ImageNet.%20A%20linear%20classifier%20trained%20on%20self-supervised%20representations%20learned%20by%20SimCLR%20achieves%2076.5%25%20top-1%20accuracy%2C%20which%20is%20a%207%25%20relative%20improvement%20over%20previous%20state-of-the-art%2C%20matching%20the%20performance%20of%20a%20supervised%20ResNet-50.%20When%20fine-tuned%20on%20only%201%25%20of%20the%20labels%2C%20we%20achieve%2085.8%25%20top-5%20accuracy%2C%20outperforming%20AlexNet%20with%20100X%20fewer%20labels.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2002.05709&amp;rft.aufirst=Ting&amp;rft.aulast=Chen&amp;rft.au=Ting%20Chen&amp;rft.au=Simon%20Kornblith&amp;rft.au=Mohammad%20Norouzi&amp;rft.au=Geoffrey%20Hinton&amp;rft.date=2020-06-30"></span>
  <div class="csl-entry">Chen, Ting, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. “Big Self-Supervised Models Are Strong Semi-Supervised Learners.” arXiv, October 25, 2020. <a href="https://doi.org/10.48550/arXiv.2006.10029">https://doi.org/10.48550/arXiv.2006.10029</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2006.10029&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Big%20Self-Supervised%20Models%20are%20Strong%20Semi-Supervised%20Learners&amp;rft.description=One%20paradigm%20for%20learning%20from%20few%20labeled%20examples%20while%20making%20best%20use%20of%20a%20large%20amount%20of%20unlabeled%20data%20is%20unsupervised%20pretraining%20followed%20by%20supervised%20fine-tuning.%20Although%20this%20paradigm%20uses%20unlabeled%20data%20in%20a%20task-agnostic%20way%2C%20in%20contrast%20to%20common%20approaches%20to%20semi-supervised%20learning%20for%20computer%20vision%2C%20we%20show%20that%20it%20is%20surprisingly%20effective%20for%20semi-supervised%20learning%20on%20ImageNet.%20A%20key%20ingredient%20of%20our%20approach%20is%20the%20use%20of%20big%20(deep%20and%20wide)%20networks%20during%20pretraining%20and%20fine-tuning.%20We%20find%20that%2C%20the%20fewer%20the%20labels%2C%20the%20more%20this%20approach%20(task-agnostic%20use%20of%20unlabeled%20data)%20benefits%20from%20a%20bigger%20network.%20After%20fine-tuning%2C%20the%20big%20network%20can%20be%20further%20improved%20and%20distilled%20into%20a%20much%20smaller%20one%20with%20little%20loss%20in%20classification%20accuracy%20by%20using%20the%20unlabeled%20examples%20for%20a%20second%20time%2C%20but%20in%20a%20task-specific%20way.%20The%20proposed%20semi-supervised%20learning%20algorithm%20can%20be%20summarized%20in%20three%20steps%3A%20unsupervised%20pretraining%20of%20a%20big%20ResNet%20model%20using%20SimCLRv2%2C%20supervised%20fine-tuning%20on%20a%20few%20labeled%20examples%2C%20and%20distillation%20with%20unlabeled%20examples%20for%20refining%20and%20transferring%20the%20task-specific%20knowledge.%20This%20procedure%20achieves%2073.9%25%20ImageNet%20top-1%20accuracy%20with%20just%201%25%20of%20the%20labels%20(%24%5Cle%2413%20labeled%20images%20per%20class)%20using%20ResNet-50%2C%20a%20%2410%5Ctimes%24%20improvement%20in%20label%20efficiency%20over%20the%20previous%20state-of-the-art.%20With%2010%25%20of%20labels%2C%20ResNet-50%20trained%20with%20our%20method%20achieves%2077.5%25%20top-1%20accuracy%2C%20outperforming%20standard%20supervised%20training%20with%20all%20of%20the%20labels.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2006.10029&amp;rft.aufirst=Ting&amp;rft.aulast=Chen&amp;rft.au=Ting%20Chen&amp;rft.au=Simon%20Kornblith&amp;rft.au=Kevin%20Swersky&amp;rft.au=Mohammad%20Norouzi&amp;rft.au=Geoffrey%20Hinton&amp;rft.date=2020-10-25"></span>
  <div class="csl-entry">Chen, Xi, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. “InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.” arXiv, June 11, 2016. <a href="https://doi.org/10.48550/arXiv.1606.03657">https://doi.org/10.48550/arXiv.1606.03657</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1606.03657&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=InfoGAN%3A%20Interpretable%20Representation%20Learning%20by%20Information%20Maximizing%20Generative%20Adversarial%20Nets&amp;rft.description=This%20paper%20describes%20InfoGAN%2C%20an%20information-theoretic%20extension%20to%20the%20Generative%20Adversarial%20Network%20that%20is%20able%20to%20learn%20disentangled%20representations%20in%20a%20completely%20unsupervised%20manner.%20InfoGAN%20is%20a%20generative%20adversarial%20network%20that%20also%20maximizes%20the%20mutual%20information%20between%20a%20small%20subset%20of%20the%20latent%20variables%20and%20the%20observation.%20We%20derive%20a%20lower%20bound%20to%20the%20mutual%20information%20objective%20that%20can%20be%20optimized%20efficiently%2C%20and%20show%20that%20our%20training%20procedure%20can%20be%20interpreted%20as%20a%20variation%20of%20the%20Wake-Sleep%20algorithm.%20Specifically%2C%20InfoGAN%20successfully%20disentangles%20writing%20styles%20from%20digit%20shapes%20on%20the%20MNIST%20dataset%2C%20pose%20from%20lighting%20of%203D%20rendered%20images%2C%20and%20background%20digits%20from%20the%20central%20digit%20on%20the%20SVHN%20dataset.%20It%20also%20discovers%20visual%20concepts%20that%20include%20hair%20styles%2C%20presence%2Fabsence%20of%20eyeglasses%2C%20and%20emotions%20on%20the%20CelebA%20face%20dataset.%20Experiments%20show%20that%20InfoGAN%20learns%20interpretable%20representations%20that%20are%20competitive%20with%20representations%20learned%20by%20existing%20fully%20supervised%20methods.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1606.03657&amp;rft.aufirst=Xi&amp;rft.aulast=Chen&amp;rft.au=Xi%20Chen&amp;rft.au=Yan%20Duan&amp;rft.au=Rein%20Houthooft&amp;rft.au=John%20Schulman&amp;rft.au=Ilya%20Sutskever&amp;rft.au=Pieter%20Abbeel&amp;rft.date=2016-06-11"></span>
  <div class="csl-entry">Creswell, Antonia, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A. Bharath. “Generative Adversarial Networks: An Overview.” <i>IEEE Signal Processing Magazine</i> 35, no. 1 (January 2018): 53–65. <a href="https://doi.org/10.1109/MSP.2017.2765202">https://doi.org/10.1109/MSP.2017.2765202</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FMSP.2017.2765202&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Generative%20Adversarial%20Networks%3A%20An%20Overview&amp;rft.jtitle=IEEE%20Signal%20Processing%20Magazine&amp;rft.volume=35&amp;rft.issue=1&amp;rft.aufirst=Antonia&amp;rft.aulast=Creswell&amp;rft.au=Antonia%20Creswell&amp;rft.au=Tom%20White&amp;rft.au=Vincent%20Dumoulin&amp;rft.au=Kai%20Arulkumaran&amp;rft.au=Biswa%20Sengupta&amp;rft.au=Anil%20A.%20Bharath&amp;rft.date=2018-01&amp;rft.pages=53-65&amp;rft.spage=53&amp;rft.epage=65&amp;rft.issn=1558-0792"></span>
  <div class="csl-entry">Croitoru, Florinel-Alin, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. “Diffusion Models in Vision: A Survey.” arXiv, September 10, 2022. <a href="https://doi.org/10.48550/arXiv.2209.04747">https://doi.org/10.48550/arXiv.2209.04747</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2209.04747&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Diffusion%20Models%20in%20Vision%3A%20A%20Survey&amp;rft.description=Denoising%20diffusion%20models%20represent%20a%20recent%20emerging%20topic%20in%20computer%20vision%2C%20demonstrating%20remarkable%20results%20in%20the%20area%20of%20generative%20modeling.%20A%20diffusion%20model%20is%20a%20deep%20generative%20model%20that%20is%20based%20on%20two%20stages%2C%20a%20forward%20diffusion%20stage%20and%20a%20reverse%20diffusion%20stage.%20In%20the%20forward%20diffusion%20stage%2C%20the%20input%20data%20is%20gradually%20perturbed%20over%20several%20steps%20by%20adding%20Gaussian%20noise.%20In%20the%20reverse%20stage%2C%20a%20model%20is%20tasked%20at%20recovering%20the%20original%20input%20data%20by%20learning%20to%20gradually%20reverse%20the%20diffusion%20process%2C%20step%20by%20step.%20Diffusion%20models%20are%20widely%20appreciated%20for%20the%20quality%20and%20diversity%20of%20the%20generated%20samples%2C%20despite%20their%20known%20computational%20burdens%2C%20i.e.%20low%20speeds%20due%20to%20the%20high%20number%20of%20steps%20involved%20during%20sampling.%20In%20this%20survey%2C%20we%20provide%20a%20comprehensive%20review%20of%20articles%20on%20denoising%20diffusion%20models%20applied%20in%20vision%2C%20comprising%20both%20theoretical%20and%20practical%20contributions%20in%20the%20field.%20First%2C%20we%20identify%20and%20present%20three%20generic%20diffusion%20modeling%20frameworks%2C%20which%20are%20based%20on%20denoising%20diffusion%20probabilistic%20models%2C%20noise%20conditioned%20score%20networks%2C%20and%20stochastic%20differential%20equations.%20We%20further%20discuss%20the%20relations%20between%20diffusion%20models%20and%20other%20deep%20generative%20models%2C%20including%20variational%20auto-encoders%2C%20generative%20adversarial%20networks%2C%20energy-based%20models%2C%20autoregressive%20models%20and%20normalizing%20flows.%20Then%2C%20we%20introduce%20a%20multi-perspective%20categorization%20of%20diffusion%20models%20applied%20in%20computer%20vision.%20Finally%2C%20we%20illustrate%20the%20current%20limitations%20of%20diffusion%20models%20and%20envision%20some%20interesting%20directions%20for%20future%20research.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2209.04747&amp;rft.aufirst=Florinel-Alin&amp;rft.aulast=Croitoru&amp;rft.au=Florinel-Alin%20Croitoru&amp;rft.au=Vlad%20Hondru&amp;rft.au=Radu%20Tudor%20Ionescu&amp;rft.au=Mubarak%20Shah&amp;rft.date=2022-09-10"></span>
  <div class="csl-entry">Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1810.04805&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=BERT%3A%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding&amp;rft.description=We%20introduce%20a%20new%20language%20representation%20model%20called%20BERT%2C%20which%20stands%20for%20Bidirectional%20Encoder%20Representations%20from%20Transformers.%20Unlike%20recent%20language%20representation%20models%2C%20BERT%20is%20designed%20to%20pre-train%20deep%20bidirectional%20representations%20from%20unlabeled%20text%20by%20jointly%20conditioning%20on%20both%20left%20and%20right%20context%20in%20all%20layers.%20As%20a%20result%2C%20the%20pre-trained%20BERT%20model%20can%20be%20fine-tuned%20with%20just%20one%20additional%20output%20layer%20to%20create%20state-of-the-art%20models%20for%20a%20wide%20range%20of%20tasks%2C%20such%20as%20question%20answering%20and%20language%20inference%2C%20without%20substantial%20task-specific%20architecture%20modifications.%20BERT%20is%20conceptually%20simple%20and%20empirically%20powerful.%20It%20obtains%20new%20state-of-the-art%20results%20on%20eleven%20natural%20language%20processing%20tasks%2C%20including%20pushing%20the%20GLUE%20score%20to%2080.5%25%20(7.7%25%20point%20absolute%20improvement)%2C%20MultiNLI%20accuracy%20to%2086.7%25%20(4.6%25%20absolute%20improvement)%2C%20SQuAD%20v1.1%20question%20answering%20Test%20F1%20to%2093.2%20(1.5%20point%20absolute%20improvement)%20and%20SQuAD%20v2.0%20Test%20F1%20to%2083.1%20(5.1%20point%20absolute%20improvement).&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1810.04805&amp;rft.aufirst=Jacob&amp;rft.aulast=Devlin&amp;rft.au=Jacob%20Devlin&amp;rft.au=Ming-Wei%20Chang&amp;rft.au=Kenton%20Lee&amp;rft.au=Kristina%20Toutanova&amp;rft.date=2019-05-24"></span>
  <div class="csl-entry">Dinh, Laurent, David Krueger, and Yoshua Bengio. “NICE: Non-Linear Independent Components Estimation.” arXiv, April 10, 2015. <a href="https://doi.org/10.48550/arXiv.1410.8516">https://doi.org/10.48550/arXiv.1410.8516</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1410.8516&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=NICE%3A%20Non-linear%20Independent%20Components%20Estimation&amp;rft.description=We%20propose%20a%20deep%20learning%20framework%20for%20modeling%20complex%20high-dimensional%20densities%20called%20Non-linear%20Independent%20Component%20Estimation%20(NICE).%20It%20is%20based%20on%20the%20idea%20that%20a%20good%20representation%20is%20one%20in%20which%20the%20data%20has%20a%20distribution%20that%20is%20easy%20to%20model.%20For%20this%20purpose%2C%20a%20non-linear%20deterministic%20transformation%20of%20the%20data%20is%20learned%20that%20maps%20it%20to%20a%20latent%20space%20so%20as%20to%20make%20the%20transformed%20data%20conform%20to%20a%20factorized%20distribution%2C%20i.e.%2C%20resulting%20in%20independent%20latent%20variables.%20We%20parametrize%20this%20transformation%20so%20that%20computing%20the%20Jacobian%20determinant%20and%20inverse%20transform%20is%20trivial%2C%20yet%20we%20maintain%20the%20ability%20to%20learn%20complex%20non-linear%20transformations%2C%20via%20a%20composition%20of%20simple%20building%20blocks%2C%20each%20based%20on%20a%20deep%20neural%20network.%20The%20training%20criterion%20is%20simply%20the%20exact%20log-likelihood%2C%20which%20is%20tractable.%20Unbiased%20ancestral%20sampling%20is%20also%20easy.%20We%20show%20that%20this%20approach%20yields%20good%20generative%20models%20on%20four%20image%20datasets%20and%20can%20be%20used%20for%20inpainting.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1410.8516&amp;rft.aufirst=Laurent&amp;rft.aulast=Dinh&amp;rft.au=Laurent%20Dinh&amp;rft.au=David%20Krueger&amp;rft.au=Yoshua%20Bengio&amp;rft.date=2015-04-10"></span>
  <div class="csl-entry">Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. “Density Estimation Using Real NVP.” arXiv, February 27, 2017. <a href="https://doi.org/10.48550/arXiv.1605.08803">https://doi.org/10.48550/arXiv.1605.08803</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1605.08803&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Density%20estimation%20using%20Real%20NVP&amp;rft.description=Unsupervised%20learning%20of%20probabilistic%20models%20is%20a%20central%20yet%20challenging%20problem%20in%20machine%20learning.%20Specifically%2C%20designing%20models%20with%20tractable%20learning%2C%20sampling%2C%20inference%20and%20evaluation%20is%20crucial%20in%20solving%20this%20task.%20We%20extend%20the%20space%20of%20such%20models%20using%20real-valued%20non-volume%20preserving%20(real%20NVP)%20transformations%2C%20a%20set%20of%20powerful%20invertible%20and%20learnable%20transformations%2C%20resulting%20in%20an%20unsupervised%20learning%20algorithm%20with%20exact%20log-likelihood%20computation%2C%20exact%20sampling%2C%20exact%20inference%20of%20latent%20variables%2C%20and%20an%20interpretable%20latent%20space.%20We%20demonstrate%20its%20ability%20to%20model%20natural%20images%20on%20four%20datasets%20through%20sampling%2C%20log-likelihood%20evaluation%20and%20latent%20variable%20manipulations.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1605.08803&amp;rft.aufirst=Laurent&amp;rft.aulast=Dinh&amp;rft.au=Laurent%20Dinh&amp;rft.au=Jascha%20Sohl-Dickstein&amp;rft.au=Samy%20Bengio&amp;rft.date=2017-02-27"></span>
  <div class="csl-entry">Doersch, Carl, Abhinav Gupta, and Alexei A. Efros. “Unsupervised Visual Representation Learning by Context Prediction.” arXiv, January 16, 2016. <a href="https://doi.org/10.48550/arXiv.1505.05192">https://doi.org/10.48550/arXiv.1505.05192</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1505.05192&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Unsupervised%20Visual%20Representation%20Learning%20by%20Context%20Prediction&amp;rft.description=This%20work%20explores%20the%20use%20of%20spatial%20context%20as%20a%20source%20of%20free%20and%20plentiful%20supervisory%20signal%20for%20training%20a%20rich%20visual%20representation.%20Given%20only%20a%20large%2C%20unlabeled%20image%20collection%2C%20we%20extract%20random%20pairs%20of%20patches%20from%20each%20image%20and%20train%20a%20convolutional%20neural%20net%20to%20predict%20the%20position%20of%20the%20second%20patch%20relative%20to%20the%20first.%20We%20argue%20that%20doing%20well%20on%20this%20task%20requires%20the%20model%20to%20learn%20to%20recognize%20objects%20and%20their%20parts.%20We%20demonstrate%20that%20the%20feature%20representation%20learned%20using%20this%20within-image%20context%20indeed%20captures%20visual%20similarity%20across%20images.%20For%20example%2C%20this%20representation%20allows%20us%20to%20perform%20unsupervised%20visual%20discovery%20of%20objects%20like%20cats%2C%20people%2C%20and%20even%20birds%20from%20the%20Pascal%20VOC%202011%20detection%20dataset.%20Furthermore%2C%20we%20show%20that%20the%20learned%20ConvNet%20can%20be%20used%20in%20the%20R-CNN%20framework%20and%20provides%20a%20significant%20boost%20over%20a%20randomly-initialized%20ConvNet%2C%20resulting%20in%20state-of-the-art%20performance%20among%20algorithms%20which%20use%20only%20Pascal-provided%20training%20set%20annotations.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1505.05192&amp;rft.aufirst=Carl&amp;rft.aulast=Doersch&amp;rft.au=Carl%20Doersch&amp;rft.au=Abhinav%20Gupta&amp;rft.au=Alexei%20A.%20Efros&amp;rft.date=2016-01-16"></span>
  <div class="csl-entry">Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv, June 3, 2021. <a href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2010.11929&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=An%20Image%20is%20Worth%2016x16%20Words%3A%20Transformers%20for%20Image%20Recognition%20at%20Scale&amp;rft.description=While%20the%20Transformer%20architecture%20has%20become%20the%20de-facto%20standard%20for%20natural%20language%20processing%20tasks%2C%20its%20applications%20to%20computer%20vision%20remain%20limited.%20In%20vision%2C%20attention%20is%20either%20applied%20in%20conjunction%20with%20convolutional%20networks%2C%20or%20used%20to%20replace%20certain%20components%20of%20convolutional%20networks%20while%20keeping%20their%20overall%20structure%20in%20place.%20We%20show%20that%20this%20reliance%20on%20CNNs%20is%20not%20necessary%20and%20a%20pure%20transformer%20applied%20directly%20to%20sequences%20of%20image%20patches%20can%20perform%20very%20well%20on%20image%20classification%20tasks.%20When%20pre-trained%20on%20large%20amounts%20of%20data%20and%20transferred%20to%20multiple%20mid-sized%20or%20small%20image%20recognition%20benchmarks%20(ImageNet%2C%20CIFAR-100%2C%20VTAB%2C%20etc.)%2C%20Vision%20Transformer%20(ViT)%20attains%20excellent%20results%20compared%20to%20state-of-the-art%20convolutional%20networks%20while%20requiring%20substantially%20fewer%20computational%20resources%20to%20train.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2010.11929&amp;rft.aufirst=Alexey&amp;rft.aulast=Dosovitskiy&amp;rft.au=Alexey%20Dosovitskiy&amp;rft.au=Lucas%20Beyer&amp;rft.au=Alexander%20Kolesnikov&amp;rft.au=Dirk%20Weissenborn&amp;rft.au=Xiaohua%20Zhai&amp;rft.au=Thomas%20Unterthiner&amp;rft.au=Mostafa%20Dehghani&amp;rft.au=Matthias%20Minderer&amp;rft.au=Georg%20Heigold&amp;rft.au=Sylvain%20Gelly&amp;rft.au=Jakob%20Uszkoreit&amp;rft.au=Neil%20Houlsby&amp;rft.date=2021-06-03"></span>
  <div class="csl-entry">Du, Yilun, and Igor Mordatch. “Implicit Generation and Generalization in Energy-Based Models.” arXiv, June 29, 2020. <a href="https://doi.org/10.48550/arXiv.1903.08689">https://doi.org/10.48550/arXiv.1903.08689</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1903.08689&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Implicit%20Generation%20and%20Generalization%20in%20Energy-Based%20Models&amp;rft.description=Energy%20based%20models%20(EBMs)%20are%20appealing%20due%20to%20their%20generality%20and%20simplicity%20in%20likelihood%20modeling%2C%20but%20have%20been%20traditionally%20difficult%20to%20train.%20We%20present%20techniques%20to%20scale%20MCMC%20based%20EBM%20training%20on%20continuous%20neural%20networks%2C%20and%20we%20show%20its%20success%20on%20the%20high-dimensional%20data%20domains%20of%20ImageNet32x32%2C%20ImageNet128x128%2C%20CIFAR-10%2C%20and%20robotic%20hand%20trajectories%2C%20achieving%20better%20samples%20than%20other%20likelihood%20models%20and%20nearing%20the%20performance%20of%20contemporary%20GAN%20approaches%2C%20while%20covering%20all%20modes%20of%20the%20data.%20We%20highlight%20some%20unique%20capabilities%20of%20implicit%20generation%20such%20as%20compositionality%20and%20corrupt%20image%20reconstruction%20and%20inpainting.%20Finally%2C%20we%20show%20that%20EBMs%20are%20useful%20models%20across%20a%20wide%20variety%20of%20tasks%2C%20achieving%20state-of-the-art%20out-of-distribution%20classification%2C%20adversarially%20robust%20classification%2C%20state-of-the-art%20continual%20online%20class%20learning%2C%20and%20coherent%20long%20term%20predicted%20trajectory%20rollouts.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1903.08689&amp;rft.aufirst=Yilun&amp;rft.aulast=Du&amp;rft.au=Yilun%20Du&amp;rft.au=Igor%20Mordatch&amp;rft.date=2020-06-29"></span>
  <div class="csl-entry">Goldberg, Yoav, and Omer Levy. “Word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method.” arXiv, February 15, 2014. <a href="https://doi.org/10.48550/arXiv.1402.3722">https://doi.org/10.48550/arXiv.1402.3722</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1402.3722&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=word2vec%20Explained%3A%20deriving%20Mikolov%20et%20al.'s%20negative-sampling%20word-embedding%20method&amp;rft.description=The%20word2vec%20software%20of%20Tomas%20Mikolov%20and%20colleagues%20(https%3A%2F%2Fcode.google.com%2Fp%2Fword2vec%2F%20)%20has%20gained%20a%20lot%20of%20traction%20lately%2C%20and%20provides%20state-of-the-art%20word%20embeddings.%20The%20learning%20models%20behind%20the%20software%20are%20described%20in%20two%20research%20papers.%20We%20found%20the%20description%20of%20the%20models%20in%20these%20papers%20to%20be%20somewhat%20cryptic%20and%20hard%20to%20follow.%20While%20the%20motivations%20and%20presentation%20may%20be%20obvious%20to%20the%20neural-networks%20language-modeling%20crowd%2C%20we%20had%20to%20struggle%20quite%20a%20bit%20to%20figure%20out%20the%20rationale%20behind%20the%20equations.%20This%20note%20is%20an%20attempt%20to%20explain%20equation%20(4)%20(negative%20sampling)%20in%20%22Distributed%20Representations%20of%20Words%20and%20Phrases%20and%20their%20Compositionality%22%20by%20Tomas%20Mikolov%2C%20Ilya%20Sutskever%2C%20Kai%20Chen%2C%20Greg%20Corrado%20and%20Jeffrey%20Dean.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1402.3722&amp;rft.aufirst=Yoav&amp;rft.aulast=Goldberg&amp;rft.au=Yoav%20Goldberg&amp;rft.au=Omer%20Levy&amp;rft.date=2014-02-15"></span>
  <div class="csl-entry">Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. “Generative Adversarial Networks.” arXiv, June 10, 2014. <a href="https://doi.org/10.48550/arXiv.1406.2661">https://doi.org/10.48550/arXiv.1406.2661</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1406.2661&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Generative%20Adversarial%20Networks&amp;rft.description=We%20propose%20a%20new%20framework%20for%20estimating%20generative%20models%20via%20an%20adversarial%20process%2C%20in%20which%20we%20simultaneously%20train%20two%20models%3A%20a%20generative%20model%20G%20that%20captures%20the%20data%20distribution%2C%20and%20a%20discriminative%20model%20D%20that%20estimates%20the%20probability%20that%20a%20sample%20came%20from%20the%20training%20data%20rather%20than%20G.%20The%20training%20procedure%20for%20G%20is%20to%20maximize%20the%20probability%20of%20D%20making%20a%20mistake.%20This%20framework%20corresponds%20to%20a%20minimax%20two-player%20game.%20In%20the%20space%20of%20arbitrary%20functions%20G%20and%20D%2C%20a%20unique%20solution%20exists%2C%20with%20G%20recovering%20the%20training%20data%20distribution%20and%20D%20equal%20to%201%2F2%20everywhere.%20In%20the%20case%20where%20G%20and%20D%20are%20defined%20by%20multilayer%20perceptrons%2C%20the%20entire%20system%20can%20be%20trained%20with%20backpropagation.%20There%20is%20no%20need%20for%20any%20Markov%20chains%20or%20unrolled%20approximate%20inference%20networks%20during%20either%20training%20or%20generation%20of%20samples.%20Experiments%20demonstrate%20the%20potential%20of%20the%20framework%20through%20qualitative%20and%20quantitative%20evaluation%20of%20the%20generated%20samples.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1406.2661&amp;rft.aufirst=Ian%20J.&amp;rft.aulast=Goodfellow&amp;rft.au=Ian%20J.%20Goodfellow&amp;rft.au=Jean%20Pouget-Abadie&amp;rft.au=Mehdi%20Mirza&amp;rft.au=Bing%20Xu&amp;rft.au=David%20Warde-Farley&amp;rft.au=Sherjil%20Ozair&amp;rft.au=Aaron%20Courville&amp;rft.au=Yoshua%20Bengio&amp;rft.date=2014-06-10"></span>
  <div class="csl-entry">Graves, Alex. “Generating Sequences With Recurrent Neural Networks.” arXiv, June 5, 2014. <a href="http://arxiv.org/abs/1308.0850">http://arxiv.org/abs/1308.0850</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Generating%20Sequences%20With%20Recurrent%20Neural%20Networks&amp;rft.description=This%20paper%20shows%20how%20Long%20Short-term%20Memory%20recurrent%20neural%20networks%20can%20be%20used%20to%20generate%20complex%20sequences%20with%20long-range%20structure%2C%20simply%20by%20predicting%20one%20data%20point%20at%20a%20time.%20The%20approach%20is%20demonstrated%20for%20text%20(where%20the%20data%20are%20discrete)%20and%20online%20handwriting%20(where%20the%20data%20are%20real-valued).%20It%20is%20then%20extended%20to%20handwriting%20synthesis%20by%20allowing%20the%20network%20to%20condition%20its%20predictions%20on%20a%20text%20sequence.%20The%20resulting%20system%20is%20able%20to%20generate%20highly%20realistic%20cursive%20handwriting%20in%20a%20wide%20variety%20of%20styles.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F1308.0850&amp;rft.aufirst=Alex&amp;rft.aulast=Graves&amp;rft.au=Alex%20Graves&amp;rft.date=2014-06-05&amp;rft.language=en"></span>
  <div class="csl-entry">Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” arXiv, September 10, 2020. <a href="https://doi.org/10.48550/arXiv.2006.07733">https://doi.org/10.48550/arXiv.2006.07733</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2006.07733&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Bootstrap%20your%20own%20latent%3A%20A%20new%20approach%20to%20self-supervised%20Learning&amp;rft.description=We%20introduce%20Bootstrap%20Your%20Own%20Latent%20(BYOL)%2C%20a%20new%20approach%20to%20self-supervised%20image%20representation%20learning.%20BYOL%20relies%20on%20two%20neural%20networks%2C%20referred%20to%20as%20online%20and%20target%20networks%2C%20that%20interact%20and%20learn%20from%20each%20other.%20From%20an%20augmented%20view%20of%20an%20image%2C%20we%20train%20the%20online%20network%20to%20predict%20the%20target%20network%20representation%20of%20the%20same%20image%20under%20a%20different%20augmented%20view.%20At%20the%20same%20time%2C%20we%20update%20the%20target%20network%20with%20a%20slow-moving%20average%20of%20the%20online%20network.%20While%20state-of-the%20art%20methods%20rely%20on%20negative%20pairs%2C%20BYOL%20achieves%20a%20new%20state%20of%20the%20art%20without%20them.%20BYOL%20reaches%20%2474.3%5C%25%24%20top-1%20classification%20accuracy%20on%20ImageNet%20using%20a%20linear%20evaluation%20with%20a%20ResNet-50%20architecture%20and%20%2479.6%5C%25%24%20with%20a%20larger%20ResNet.%20We%20show%20that%20BYOL%20performs%20on%20par%20or%20better%20than%20the%20current%20state%20of%20the%20art%20on%20both%20transfer%20and%20semi-supervised%20benchmarks.%20Our%20implementation%20and%20pretrained%20models%20are%20given%20on%20GitHub.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2006.07733&amp;rft.aufirst=Jean-Bastien&amp;rft.aulast=Grill&amp;rft.au=Jean-Bastien%20Grill&amp;rft.au=Florian%20Strub&amp;rft.au=Florent%20Altch%C3%A9&amp;rft.au=Corentin%20Tallec&amp;rft.au=Pierre%20H.%20Richemond&amp;rft.au=Elena%20Buchatskaya&amp;rft.au=Carl%20Doersch&amp;rft.au=Bernardo%20Avila%20Pires&amp;rft.au=Zhaohan%20Daniel%20Guo&amp;rft.au=Mohammad%20Gheshlaghi%20Azar&amp;rft.au=Bilal%20Piot&amp;rft.au=Koray%20Kavukcuoglu&amp;rft.au=R%C3%A9mi%20Munos&amp;rft.au=Michal%20Valko&amp;rft.date=2020-09-10"></span>
  <div class="csl-entry">He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. “Masked Autoencoders Are Scalable Vision Learners.” arXiv, December 19, 2021. <a href="https://doi.org/10.48550/arXiv.2111.06377">https://doi.org/10.48550/arXiv.2111.06377</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2111.06377&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners&amp;rft.description=This%20paper%20shows%20that%20masked%20autoencoders%20(MAE)%20are%20scalable%20self-supervised%20learners%20for%20computer%20vision.%20Our%20MAE%20approach%20is%20simple%3A%20we%20mask%20random%20patches%20of%20the%20input%20image%20and%20reconstruct%20the%20missing%20pixels.%20It%20is%20based%20on%20two%20core%20designs.%20First%2C%20we%20develop%20an%20asymmetric%20encoder-decoder%20architecture%2C%20with%20an%20encoder%20that%20operates%20only%20on%20the%20visible%20subset%20of%20patches%20(without%20mask%20tokens)%2C%20along%20with%20a%20lightweight%20decoder%20that%20reconstructs%20the%20original%20image%20from%20the%20latent%20representation%20and%20mask%20tokens.%20Second%2C%20we%20find%20that%20masking%20a%20high%20proportion%20of%20the%20input%20image%2C%20e.g.%2C%2075%25%2C%20yields%20a%20nontrivial%20and%20meaningful%20self-supervisory%20task.%20Coupling%20these%20two%20designs%20enables%20us%20to%20train%20large%20models%20efficiently%20and%20effectively%3A%20we%20accelerate%20training%20(by%203x%20or%20more)%20and%20improve%20accuracy.%20Our%20scalable%20approach%20allows%20for%20learning%20high-capacity%20models%20that%20generalize%20well%3A%20e.g.%2C%20a%20vanilla%20ViT-Huge%20model%20achieves%20the%20best%20accuracy%20(87.8%25)%20among%20methods%20that%20use%20only%20ImageNet-1K%20data.%20Transfer%20performance%20in%20downstream%20tasks%20outperforms%20supervised%20pre-training%20and%20shows%20promising%20scaling%20behavior.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2111.06377&amp;rft.aufirst=Kaiming&amp;rft.aulast=He&amp;rft.au=Kaiming%20He&amp;rft.au=Xinlei%20Chen&amp;rft.au=Saining%20Xie&amp;rft.au=Yanghao%20Li&amp;rft.au=Piotr%20Doll%C3%A1r&amp;rft.au=Ross%20Girshick&amp;rft.date=2021-12-19"></span>
  <div class="csl-entry">OpenAI. “Implicit Generation and Generalization Methods for Energy-Based Models,” March 21, 2019. <a href="https://openai.com/blog/energy-based-models/">https://openai.com/blog/energy-based-models/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Implicit%20Generation%20and%20Generalization%20Methods%20for%20Energy-Based%20Models&amp;rft.description=We%E2%80%99ve%20made%20progress%20towards%20stable%20and%20scalable%20training%20of%20energy-based%20models%20(EBMs)%20resulting%20in%20better%20sample%20quality%20and%20generalization%20ability%20than%20existing%20models.%20Generation%20in%20EBMs%20spends%20more%20compute%20to%20continually%20refine%20its%20answers%20and%20doing%20so%20can%20generate%20samples%20competitive%20with%20GANs%20at%20low%20temperatures%2C%20while%20also%20having%20mode%20coverage%20guarantees%20of%20likelihood-based%20models.%20We%20hope%20these%20findings%20stimulate%20further%20research%20into%20this%20promising%20class%20of%20models.&amp;rft.identifier=https%3A%2F%2Fopenai.com%2Fblog%2Fenergy-based-models%2F&amp;rft.date=2019-03-21&amp;rft.language=en"></span>
  <div class="csl-entry">Jang, Eric, Shixiang Gu, and Ben Poole. “Categorical Reparameterization with Gumbel-Softmax.” arXiv, August 5, 2017. <a href="https://doi.org/10.48550/arXiv.1611.01144">https://doi.org/10.48550/arXiv.1611.01144</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1611.01144&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Categorical%20Reparameterization%20with%20Gumbel-Softmax&amp;rft.description=Categorical%20variables%20are%20a%20natural%20choice%20for%20representing%20discrete%20structure%20in%20the%20world.%20However%2C%20stochastic%20neural%20networks%20rarely%20use%20categorical%20latent%20variables%20due%20to%20the%20inability%20to%20backpropagate%20through%20samples.%20In%20this%20work%2C%20we%20present%20an%20efficient%20gradient%20estimator%20that%20replaces%20the%20non-differentiable%20sample%20from%20a%20categorical%20distribution%20with%20a%20differentiable%20sample%20from%20a%20novel%20Gumbel-Softmax%20distribution.%20This%20distribution%20has%20the%20essential%20property%20that%20it%20can%20be%20smoothly%20annealed%20into%20a%20categorical%20distribution.%20We%20show%20that%20our%20Gumbel-Softmax%20estimator%20outperforms%20state-of-the-art%20gradient%20estimators%20on%20structured%20output%20prediction%20and%20unsupervised%20generative%20modeling%20tasks%20with%20categorical%20latent%20variables%2C%20and%20enables%20large%20speedups%20on%20semi-supervised%20classification.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1611.01144&amp;rft.aufirst=Eric&amp;rft.aulast=Jang&amp;rft.au=Eric%20Jang&amp;rft.au=Shixiang%20Gu&amp;rft.au=Ben%20Poole&amp;rft.date=2017-08-05"></span>
  <div class="csl-entry">Kingma, Diederik P., and Max Welling. “An Introduction to Variational Autoencoders.” <i>Foundations and Trends® in Machine Learning</i> 12, no. 4 (2019): 307–92. <a href="https://doi.org/10.1561/2200000056">https://doi.org/10.1561/2200000056</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1561%2F2200000056&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=An%20Introduction%20to%20Variational%20Autoencoders&amp;rft.jtitle=Foundations%20and%20Trends%C2%AE%20in%20Machine%20Learning&amp;rft.stitle=FNT%20in%20Machine%20Learning&amp;rft.volume=12&amp;rft.issue=4&amp;rft.aufirst=Diederik%20P.&amp;rft.aulast=Kingma&amp;rft.au=Diederik%20P.%20Kingma&amp;rft.au=Max%20Welling&amp;rft.date=2019&amp;rft.pages=307-392&amp;rft.spage=307&amp;rft.epage=392&amp;rft.issn=1935-8237%2C%201935-8245"></span>
  <div class="csl-entry">———. “An Introduction to Variational Autoencoders.” <i>Foundations and Trends® in Machine Learning</i> 12, no. 4 (2019): 307–92. <a href="https://doi.org/10.1561/2200000056">https://doi.org/10.1561/2200000056</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1561%2F2200000056&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=An%20Introduction%20to%20Variational%20Autoencoders&amp;rft.jtitle=Foundations%20and%20Trends%C2%AE%20in%20Machine%20Learning&amp;rft.stitle=FNT%20in%20Machine%20Learning&amp;rft.volume=12&amp;rft.issue=4&amp;rft.aufirst=Diederik%20P.&amp;rft.aulast=Kingma&amp;rft.au=Diederik%20P.%20Kingma&amp;rft.au=Max%20Welling&amp;rft.date=2019&amp;rft.pages=307-392&amp;rft.spage=307&amp;rft.epage=392&amp;rft.issn=1935-8237%2C%201935-8245"></span>
  <div class="csl-entry">———. “Auto-Encoding Variational Bayes.” arXiv, May 1, 2014. <a href="https://doi.org/10.48550/arXiv.1312.6114">https://doi.org/10.48550/arXiv.1312.6114</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1312.6114&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Auto-Encoding%20Variational%20Bayes&amp;rft.description=How%20can%20we%20perform%20efficient%20inference%20and%20learning%20in%20directed%20probabilistic%20models%2C%20in%20the%20presence%20of%20continuous%20latent%20variables%20with%20intractable%20posterior%20distributions%2C%20and%20large%20datasets%3F%20We%20introduce%20a%20stochastic%20variational%20inference%20and%20learning%20algorithm%20that%20scales%20to%20large%20datasets%20and%2C%20under%20some%20mild%20differentiability%20conditions%2C%20even%20works%20in%20the%20intractable%20case.%20Our%20contributions%20is%20two-fold.%20First%2C%20we%20show%20that%20a%20reparameterization%20of%20the%20variational%20lower%20bound%20yields%20a%20lower%20bound%20estimator%20that%20can%20be%20straightforwardly%20optimized%20using%20standard%20stochastic%20gradient%20methods.%20Second%2C%20we%20show%20that%20for%20i.i.d.%20datasets%20with%20continuous%20latent%20variables%20per%20datapoint%2C%20posterior%20inference%20can%20be%20made%20especially%20efficient%20by%20fitting%20an%20approximate%20inference%20model%20(also%20called%20a%20recognition%20model)%20to%20the%20intractable%20posterior%20using%20the%20proposed%20lower%20bound%20estimator.%20Theoretical%20advantages%20are%20reflected%20in%20experimental%20results.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1312.6114&amp;rft.aufirst=Diederik%20P.&amp;rft.aulast=Kingma&amp;rft.au=Diederik%20P.%20Kingma&amp;rft.au=Max%20Welling&amp;rft.date=2014-05-01"></span>
  <div class="csl-entry">Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. “Siamese Neural Networks for One-Shot Image Recognition,” n.d., 8.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Siamese%20Neural%20Networks%20for%20One-shot%20Image%20Recognition&amp;rft.aufirst=Gregory&amp;rft.aulast=Koch&amp;rft.au=Gregory%20Koch&amp;rft.au=Richard%20Zemel&amp;rft.au=Ruslan%20Salakhutdinov&amp;rft.pages=8&amp;rft.language=en"></span>
  <div class="csl-entry">LeCun, Yann. “Learning Invariant Feature Hierarchies.” In <i>Computer Vision – ECCV 2012. Workshops and Demonstrations</i>, edited by Andrea Fusiello, Vittorio Murino, and Rita Cucchiara, 7583:496–505. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012. <a href="https://doi.org/10.1007/978-3-642-33863-2_51">https://doi.org/10.1007/978-3-642-33863-2_51</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-3-642-33862-5%20978-3-642-33863-2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Learning%20Invariant%20Feature%20Hierarchies&amp;rft.place=Berlin%2C%20Heidelberg&amp;rft.publisher=Springer%20Berlin%20Heidelberg&amp;rft.aufirst=Yann&amp;rft.aulast=LeCun&amp;rft.au=David%20Hutchison&amp;rft.au=Takeo%20Kanade&amp;rft.au=Josef%20Kittler&amp;rft.au=Jon%20M.%20Kleinberg&amp;rft.au=Friedemann%20Mattern&amp;rft.au=John%20C.%20Mitchell&amp;rft.au=Moni%20Naor&amp;rft.au=Oscar%20Nierstrasz&amp;rft.au=C.%20Pandu%20Rangan&amp;rft.au=Bernhard%20Steffen&amp;rft.au=Madhu%20Sudan&amp;rft.au=Demetri%20Terzopoulos&amp;rft.au=Doug%20Tygar&amp;rft.au=Moshe%20Y.%20Vardi&amp;rft.au=Gerhard%20Weikum&amp;rft.au=Andrea%20Fusiello&amp;rft.au=Vittorio%20Murino&amp;rft.au=Rita%20Cucchiara&amp;rft.au=Yann%20LeCun&amp;rft.date=2012&amp;rft.pages=496-505&amp;rft.spage=496&amp;rft.epage=505&amp;rft.isbn=978-3-642-33862-5%20978-3-642-33863-2&amp;rft.language=en"></span>
  <div class="csl-entry">Makansi, Osama, Eddy Ilg, Özgün Cicek, and Thomas Brox. “Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction.” In <i>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 7137–46, 2019. <a href="https://doi.org/10.1109/CVPR.2019.00731">https://doi.org/10.1109/CVPR.2019.00731</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FCVPR.2019.00731&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Overcoming%20Limitations%20of%20Mixture%20Density%20Networks%3A%20A%20Sampling%20and%20Fitting%20Framework%20for%20Multimodal%20Future%20Prediction&amp;rft.btitle=2019%20IEEE%2FCVF%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20(CVPR)&amp;rft.aufirst=Osama&amp;rft.aulast=Makansi&amp;rft.au=Osama%20Makansi&amp;rft.au=Eddy%20Ilg&amp;rft.au=%C3%96zg%C3%BCn%20Cicek&amp;rft.au=Thomas%20Brox&amp;rft.date=2019-06&amp;rft.pages=7137-7146&amp;rft.spage=7137&amp;rft.epage=7146"></span>
  <div class="csl-entry">McCann, Bryan, James Bradbury, Caiming Xiong, and Richard Socher. “Learned in Translation: Contextualized Word Vectors.” arXiv, June 20, 2018. <a href="https://doi.org/10.48550/arXiv.1708.00107">https://doi.org/10.48550/arXiv.1708.00107</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1708.00107&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Learned%20in%20Translation%3A%20Contextualized%20Word%20Vectors&amp;rft.description=Computer%20vision%20has%20benefited%20from%20initializing%20multiple%20deep%20layers%20with%20weights%20pretrained%20on%20large%20supervised%20training%20sets%20like%20ImageNet.%20Natural%20language%20processing%20(NLP)%20typically%20sees%20initialization%20of%20only%20the%20lowest%20layer%20of%20deep%20models%20with%20pretrained%20word%20vectors.%20In%20this%20paper%2C%20we%20use%20a%20deep%20LSTM%20encoder%20from%20an%20attentional%20sequence-to-sequence%20model%20trained%20for%20machine%20translation%20(MT)%20to%20contextualize%20word%20vectors.%20We%20show%20that%20adding%20these%20context%20vectors%20(CoVe)%20improves%20performance%20over%20using%20only%20unsupervised%20word%20and%20character%20vectors%20on%20a%20wide%20variety%20of%20common%20NLP%20tasks%3A%20sentiment%20analysis%20(SST%2C%20IMDb)%2C%20question%20classification%20(TREC)%2C%20entailment%20(SNLI)%2C%20and%20question%20answering%20(SQuAD).%20For%20fine-grained%20sentiment%20analysis%20and%20entailment%2C%20CoVe%20improves%20performance%20of%20our%20baseline%20models%20to%20the%20state%20of%20the%20art.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1708.00107&amp;rft.aufirst=Bryan&amp;rft.aulast=McCann&amp;rft.au=Bryan%20McCann&amp;rft.au=James%20Bradbury&amp;rft.au=Caiming%20Xiong&amp;rft.au=Richard%20Socher&amp;rft.date=2018-06-20"></span>
  <div class="csl-entry">Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space.” arXiv, September 6, 2013. <a href="https://doi.org/10.48550/arXiv.1301.3781">https://doi.org/10.48550/arXiv.1301.3781</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1301.3781&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space&amp;rft.description=We%20propose%20two%20novel%20model%20architectures%20for%20computing%20continuous%20vector%20representations%20of%20words%20from%20very%20large%20data%20sets.%20The%20quality%20of%20these%20representations%20is%20measured%20in%20a%20word%20similarity%20task%2C%20and%20the%20results%20are%20compared%20to%20the%20previously%20best%20performing%20techniques%20based%20on%20different%20types%20of%20neural%20networks.%20We%20observe%20large%20improvements%20in%20accuracy%20at%20much%20lower%20computational%20cost%2C%20i.e.%20it%20takes%20less%20than%20a%20day%20to%20learn%20high%20quality%20word%20vectors%20from%20a%201.6%20billion%20words%20data%20set.%20Furthermore%2C%20we%20show%20that%20these%20vectors%20provide%20state-of-the-art%20performance%20on%20our%20test%20set%20for%20measuring%20syntactic%20and%20semantic%20word%20similarities.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1301.3781&amp;rft.aufirst=Tomas&amp;rft.aulast=Mikolov&amp;rft.au=Tomas%20Mikolov&amp;rft.au=Kai%20Chen&amp;rft.au=Greg%20Corrado&amp;rft.au=Jeffrey%20Dean&amp;rft.date=2013-09-06"></span>
  <div class="csl-entry">Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. “Neural Discrete Representation Learning.” arXiv, May 30, 2018. <a href="http://arxiv.org/abs/1711.00937">http://arxiv.org/abs/1711.00937</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Neural%20Discrete%20Representation%20Learning&amp;rft.description=Learning%20useful%20representations%20without%20supervision%20remains%20a%20key%20challenge%20in%20machine%20learning.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20powerful%20generative%20model%20that%20learns%20such%20discrete%20representations.%20Our%20model%2C%20the%20Vector%20QuantisedVariational%20AutoEncoder%20(VQ-VAE)%2C%20differs%20from%20VAEs%20in%20two%20key%20ways%3A%20the%20encoder%20network%20outputs%20discrete%2C%20rather%20than%20continuous%2C%20codes%3B%20and%20the%20prior%20is%20learnt%20rather%20than%20static.%20In%20order%20to%20learn%20a%20discrete%20latent%20representation%2C%20we%20incorporate%20ideas%20from%20vector%20quantisation%20(VQ).%20Using%20the%20VQ%20method%20allows%20the%20model%20to%20circumvent%20issues%20of%20%E2%80%9Cposterior%20collapse%E2%80%9D%20-%E2%80%94%20where%20the%20latents%20are%20ignored%20when%20they%20are%20paired%20with%20a%20powerful%20autoregressive%20decoder%20-%E2%80%94%20typically%20observed%20in%20the%20VAE%20framework.%20Pairing%20these%20representations%20with%20an%20autoregressive%20prior%2C%20the%20model%20can%20generate%20high%20quality%20images%2C%20videos%2C%20and%20speech%20as%20well%20as%20doing%20high%20quality%20speaker%20conversion%20and%20unsupervised%20learning%20of%20phonemes%2C%20providing%20further%20evidence%20of%20the%20utility%20of%20the%20learnt%20representations.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F1711.00937&amp;rft.aufirst=Aaron%20van%20den&amp;rft.aulast=Oord&amp;rft.au=Aaron%20van%20den%20Oord&amp;rft.au=Oriol%20Vinyals&amp;rft.au=Koray%20Kavukcuoglu&amp;rft.date=2018-05-30&amp;rft.language=en"></span>
  <div class="csl-entry">Papamakarios, George, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. “Normalizing Flows for Probabilistic Modeling and Inference.” arXiv, April 8, 2021. <a href="https://doi.org/10.48550/arXiv.1912.02762">https://doi.org/10.48550/arXiv.1912.02762</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1912.02762&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Normalizing%20Flows%20for%20Probabilistic%20Modeling%20and%20Inference&amp;rft.description=Normalizing%20flows%20provide%20a%20general%20mechanism%20for%20defining%20expressive%20probability%20distributions%2C%20only%20requiring%20the%20specification%20of%20a%20(usually%20simple)%20base%20distribution%20and%20a%20series%20of%20bijective%20transformations.%20There%20has%20been%20much%20recent%20work%20on%20normalizing%20flows%2C%20ranging%20from%20improving%20their%20expressive%20power%20to%20expanding%20their%20application.%20We%20believe%20the%20field%20has%20now%20matured%20and%20is%20in%20need%20of%20a%20unified%20perspective.%20In%20this%20review%2C%20we%20attempt%20to%20provide%20such%20a%20perspective%20by%20describing%20flows%20through%20the%20lens%20of%20probabilistic%20modeling%20and%20inference.%20We%20place%20special%20emphasis%20on%20the%20fundamental%20principles%20of%20flow%20design%2C%20and%20discuss%20foundational%20topics%20such%20as%20expressive%20power%20and%20computational%20trade-offs.%20We%20also%20broaden%20the%20conceptual%20framing%20of%20flows%20by%20relating%20them%20to%20more%20general%20probability%20transformations.%20Lastly%2C%20we%20summarize%20the%20use%20of%20flows%20for%20tasks%20such%20as%20generative%20modeling%2C%20approximate%20inference%2C%20and%20supervised%20learning.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1912.02762&amp;rft.aufirst=George&amp;rft.aulast=Papamakarios&amp;rft.au=George%20Papamakarios&amp;rft.au=Eric%20Nalisnick&amp;rft.au=Danilo%20Jimenez%20Rezende&amp;rft.au=Shakir%20Mohamed&amp;rft.au=Balaji%20Lakshminarayanan&amp;rft.date=2021-04-08"></span>
  <div class="csl-entry">Park, Taesung, Alexei A. Efros, Richard Zhang, and Jun-Yan Zhu. “Contrastive Learning for Unpaired Image-to-Image Translation.” arXiv, August 20, 2020. <a href="https://doi.org/10.48550/arXiv.2007.15651">https://doi.org/10.48550/arXiv.2007.15651</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.2007.15651&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Contrastive%20Learning%20for%20Unpaired%20Image-to-Image%20Translation&amp;rft.description=In%20image-to-image%20translation%2C%20each%20patch%20in%20the%20output%20should%20reflect%20the%20content%20of%20the%20corresponding%20patch%20in%20the%20input%2C%20independent%20of%20domain.%20We%20propose%20a%20straightforward%20method%20for%20doing%20so%20--%20maximizing%20mutual%20information%20between%20the%20two%2C%20using%20a%20framework%20based%20on%20contrastive%20learning.%20The%20method%20encourages%20two%20elements%20(corresponding%20patches)%20to%20map%20to%20a%20similar%20point%20in%20a%20learned%20feature%20space%2C%20relative%20to%20other%20elements%20(other%20patches)%20in%20the%20dataset%2C%20referred%20to%20as%20negatives.%20We%20explore%20several%20critical%20design%20choices%20for%20making%20contrastive%20learning%20effective%20in%20the%20image%20synthesis%20setting.%20Notably%2C%20we%20use%20a%20multilayer%2C%20patch-based%20approach%2C%20rather%20than%20operate%20on%20entire%20images.%20Furthermore%2C%20we%20draw%20negatives%20from%20within%20the%20input%20image%20itself%2C%20rather%20than%20from%20the%20rest%20of%20the%20dataset.%20We%20demonstrate%20that%20our%20framework%20enables%20one-sided%20translation%20in%20the%20unpaired%20image-to-image%20translation%20setting%2C%20while%20improving%20quality%20and%20reducing%20training%20time.%20In%20addition%2C%20our%20method%20can%20even%20be%20extended%20to%20the%20training%20setting%20where%20each%20%22domain%22%20is%20only%20a%20single%20image.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.2007.15651&amp;rft.aufirst=Taesung&amp;rft.aulast=Park&amp;rft.au=Taesung%20Park&amp;rft.au=Alexei%20A.%20Efros&amp;rft.au=Richard%20Zhang&amp;rft.au=Jun-Yan%20Zhu&amp;rft.date=2020-08-20"></span>
  <div class="csl-entry">Pathak, Deepak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. “Context Encoders: Feature Learning by Inpainting.” arXiv, November 21, 2016. <a href="https://doi.org/10.48550/arXiv.1604.07379">https://doi.org/10.48550/arXiv.1604.07379</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1604.07379&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Context%20Encoders%3A%20Feature%20Learning%20by%20Inpainting&amp;rft.description=We%20present%20an%20unsupervised%20visual%20feature%20learning%20algorithm%20driven%20by%20context-based%20pixel%20prediction.%20By%20analogy%20with%20auto-encoders%2C%20we%20propose%20Context%20Encoders%20--%20a%20convolutional%20neural%20network%20trained%20to%20generate%20the%20contents%20of%20an%20arbitrary%20image%20region%20conditioned%20on%20its%20surroundings.%20In%20order%20to%20succeed%20at%20this%20task%2C%20context%20encoders%20need%20to%20both%20understand%20the%20content%20of%20the%20entire%20image%2C%20as%20well%20as%20produce%20a%20plausible%20hypothesis%20for%20the%20missing%20part(s).%20When%20training%20context%20encoders%2C%20we%20have%20experimented%20with%20both%20a%20standard%20pixel-wise%20reconstruction%20loss%2C%20as%20well%20as%20a%20reconstruction%20plus%20an%20adversarial%20loss.%20The%20latter%20produces%20much%20sharper%20results%20because%20it%20can%20better%20handle%20multiple%20modes%20in%20the%20output.%20We%20found%20that%20a%20context%20encoder%20learns%20a%20representation%20that%20captures%20not%20just%20appearance%20but%20also%20the%20semantics%20of%20visual%20structures.%20We%20quantitatively%20demonstrate%20the%20effectiveness%20of%20our%20learned%20features%20for%20CNN%20pre-training%20on%20classification%2C%20detection%2C%20and%20segmentation%20tasks.%20Furthermore%2C%20context%20encoders%20can%20be%20used%20for%20semantic%20inpainting%20tasks%2C%20either%20stand-alone%20or%20as%20initialization%20for%20non-parametric%20methods.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1604.07379&amp;rft.aufirst=Deepak&amp;rft.aulast=Pathak&amp;rft.au=Deepak%20Pathak&amp;rft.au=Philipp%20Krahenbuhl&amp;rft.au=Jeff%20Donahue&amp;rft.au=Trevor%20Darrell&amp;rft.au=Alexei%20A.%20Efros&amp;rft.date=2016-11-21"></span>
  <div class="csl-entry">Pennington, Jeffrey, Richard Socher, and Christopher Manning. “Glove: Global Vectors for Word Representation.” In <i>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, 1532–43. Doha, Qatar: Association for Computational Linguistics, 2014. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.3115%2Fv1%2FD14-1162&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Glove%3A%20Global%20Vectors%20for%20Word%20Representation&amp;rft.btitle=Proceedings%20of%20the%202014%20Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing%20(EMNLP)&amp;rft.place=Doha%2C%20Qatar&amp;rft.publisher=Association%20for%20Computational%20Linguistics&amp;rft.aufirst=Jeffrey&amp;rft.aulast=Pennington&amp;rft.au=Jeffrey%20Pennington&amp;rft.au=Richard%20Socher&amp;rft.au=Christopher%20Manning&amp;rft.date=2014&amp;rft.pages=1532-1543&amp;rft.spage=1532&amp;rft.epage=1543&amp;rft.language=en"></span>
  <div class="csl-entry">Pentland, Alex. “E i g e d c e s for Recognition,” n.d., 16.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=E%20i%20g%20e%20d%20c%20e%20s%20for%20Recognition&amp;rft.aufirst=Alex&amp;rft.aulast=Pentland&amp;rft.au=Alex%20Pentland&amp;rft.pages=16&amp;rft.language=en"></span>
  <div class="csl-entry">Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. “Deep Contextualized Word Representations.” arXiv, March 22, 2018. <a href="http://arxiv.org/abs/1802.05365">http://arxiv.org/abs/1802.05365</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Deep%20contextualized%20word%20representations&amp;rft.description=We%20introduce%20a%20new%20type%20of%20deep%20contextualized%20word%20representation%20that%20models%20both%20(1)%20complex%20characteristics%20of%20word%20use%20(e.g.%2C%20syntax%20and%20semantics)%2C%20and%20(2)%20how%20these%20uses%20vary%20across%20linguistic%20contexts%20(i.e.%2C%20to%20model%20polysemy).%20Our%20word%20vectors%20are%20learned%20functions%20of%20the%20internal%20states%20of%20a%20deep%20bidirectional%20language%20model%20(biLM)%2C%20which%20is%20pre-trained%20on%20a%20large%20text%20corpus.%20We%20show%20that%20these%20representations%20can%20be%20easily%20added%20to%20existing%20models%20and%20significantly%20improve%20the%20state%20of%20the%20art%20across%20six%20challenging%20NLP%20problems%2C%20including%20question%20answering%2C%20textual%20entailment%20and%20sentiment%20analysis.%20We%20also%20present%20an%20analysis%20showing%20that%20exposing%20the%20deep%20internals%20of%20the%20pre-trained%20network%20is%20crucial%2C%20allowing%20downstream%20models%20to%20mix%20different%20types%20of%20semi-supervision%20signals.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F1802.05365&amp;rft.aufirst=Matthew%20E.&amp;rft.aulast=Peters&amp;rft.au=Matthew%20E.%20Peters&amp;rft.au=Mark%20Neumann&amp;rft.au=Mohit%20Iyyer&amp;rft.au=Matt%20Gardner&amp;rft.au=Christopher%20Clark&amp;rft.au=Kenton%20Lee&amp;rft.au=Luke%20Zettlemoyer&amp;rft.date=2018-03-22"></span>
  <div class="csl-entry">Principe, Jose C., Dongxin Xu, and John W. Fisher Iii. “Information-Theoretic Learning,” 1999.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=document&amp;rft.title=Information-Theoretic%20Learning&amp;rft.description=This%20chapter%20seeks%20to%20extend%20the%20ubiquitous%20mean-square%20error%20criterion%20(MSE)%20to%20cost%20functions%20that%20include%20more%20information%20about%20the%20training%20data.%20Since%20the%20learning%20process%20ultimately%20should%20transfer%20the%20information%20carried%20in%20the%20data%20samples%20onto%20the%20system's%20parameters%2C%20a%20natural%20goal%20is%20to%20find%20cost%20functions%20that%20directly%20manipulate%20information.%20Hence%20the%20name%20informationtheoretic%20learning%20(ITL).%20In%20order%20to%20be%20useful%2C%20ITL%20should%20be%20independent%20of%20the%20learning%20machine%20architecture%2C%20and%20require%20solely%20the%20availability%20of%20the%20data%2C%20i.e.%20it%20should%20not%20require%20a%20priori%20assumptions%20about%20the%20data%20distributions.%20The%20chapter%20presents%20our%20current%20efforts%20to%20develop%20ITL%20criteria%20based%20on%20the%20integration%20of%20nonparametric%20density%20estimators%20with%20Renyi's%20quadratic%20entropy%20definition.%20As%20a%20motivation%20we%20start%20with%20an%20application%20of%20the%20MSE%20to%20manipulate%20information%20using%20the%20nonlinear%20characteristics%20of%20the%20learning%20machine.%20This%20section%20illustrates%20the%20issues%20faced%20when%20we%20attempt%20to%20use...&amp;rft.aufirst=Jose%20C.&amp;rft.aulast=Principe&amp;rft.au=Jose%20C.%20Principe&amp;rft.au=Dongxin%20Xu&amp;rft.au=John%20W.%20Fisher%20Iii&amp;rft.date=1999"></span>
  <div class="csl-entry">Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. “Improving Language Understanding by Generative Pre-Training,” n.d., 12.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Improving%20Language%20Understanding%20by%20Generative%20Pre-Training&amp;rft.aufirst=Alec&amp;rft.aulast=Radford&amp;rft.au=Alec%20Radford&amp;rft.au=Karthik%20Narasimhan&amp;rft.au=Tim%20Salimans&amp;rft.au=Ilya%20Sutskever&amp;rft.pages=12&amp;rft.language=en"></span>
  <div class="csl-entry">Razavi, Ali, Aaron van den Oord, and Oriol Vinyals. “Generating Diverse High-Fidelity Images with VQ-VAE-2.” arXiv, June 2, 2019. <a href="https://doi.org/10.48550/arXiv.1906.00446">https://doi.org/10.48550/arXiv.1906.00446</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1906.00446&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Generating%20Diverse%20High-Fidelity%20Images%20with%20VQ-VAE-2&amp;rft.description=We%20explore%20the%20use%20of%20Vector%20Quantized%20Variational%20AutoEncoder%20(VQ-VAE)%20models%20for%20large%20scale%20image%20generation.%20To%20this%20end%2C%20we%20scale%20and%20enhance%20the%20autoregressive%20priors%20used%20in%20VQ-VAE%20to%20generate%20synthetic%20samples%20of%20much%20higher%20coherence%20and%20fidelity%20than%20possible%20before.%20We%20use%20simple%20feed-forward%20encoder%20and%20decoder%20networks%2C%20making%20our%20model%20an%20attractive%20candidate%20for%20applications%20where%20the%20encoding%20and%2For%20decoding%20speed%20is%20critical.%20Additionally%2C%20VQ-VAE%20requires%20sampling%20an%20autoregressive%20model%20only%20in%20the%20compressed%20latent%20space%2C%20which%20is%20an%20order%20of%20magnitude%20faster%20than%20sampling%20in%20the%20pixel%20space%2C%20especially%20for%20large%20images.%20We%20demonstrate%20that%20a%20multi-scale%20hierarchical%20organization%20of%20VQ-VAE%2C%20augmented%20with%20powerful%20priors%20over%20the%20latent%20codes%2C%20is%20able%20to%20generate%20samples%20with%20quality%20that%20rivals%20that%20of%20state%20of%20the%20art%20Generative%20Adversarial%20Networks%20on%20multifaceted%20datasets%20such%20as%20ImageNet%2C%20while%20not%20suffering%20from%20GAN's%20known%20shortcomings%20such%20as%20mode%20collapse%20and%20lack%20of%20diversity.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1906.00446&amp;rft.aufirst=Ali&amp;rft.aulast=Razavi&amp;rft.au=Ali%20Razavi&amp;rft.au=Aaron%20van%20den%20Oord&amp;rft.au=Oriol%20Vinyals&amp;rft.date=2019-06-02"></span>
  <div class="csl-entry">Rocca, Joseph. “Understanding Variational Autoencoders (VAEs).” Medium, March 21, 2021. <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Understanding%20Variational%20Autoencoders%20(VAEs)&amp;rft.description=Building%2C%20step%20by%20step%2C%20the%20reasoning%20that%20leads%20to%20VAEs.&amp;rft.identifier=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-variational-autoencoders-vaes-f70510919f73&amp;rft.aufirst=Joseph&amp;rft.aulast=Rocca&amp;rft.au=Joseph%20Rocca&amp;rft.date=2021-03-21&amp;rft.language=en"></span>
  <div class="csl-entry">Shree, Priya. “The Journey of Open AI GPT Models.” <i>Walmart Global Tech Blog</i> (blog), November 10, 2020. <a href="https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2">https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=The%20Journey%20of%20Open%20AI%20GPT%20models&amp;rft.description=Generative%20Pre-trained%20Transformer%20(GPT)%20models%20by%20OpenAI%20have%20taken%20natural%20language%20processing%20(NLP)%20community%20by%20storm%20by%20introducing%E2%80%A6&amp;rft.identifier=https%3A%2F%2Fmedium.com%2Fwalmartglobaltech%2Fthe-journey-of-open-ai-gpt-models-32d95b7b7fb2&amp;rft.aufirst=Priya&amp;rft.aulast=Shree&amp;rft.au=Priya%20Shree&amp;rft.date=2020-11-10&amp;rft.language=en"></span>
  <div class="csl-entry">Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, December 5, 2017. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1706.03762&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Attention%20Is%20All%20You%20Need&amp;rft.description=The%20dominant%20sequence%20transduction%20models%20are%20based%20on%20complex%20recurrent%20or%20convolutional%20neural%20networks%20in%20an%20encoder-decoder%20configuration.%20The%20best%20performing%20models%20also%20connect%20the%20encoder%20and%20decoder%20through%20an%20attention%20mechanism.%20We%20propose%20a%20new%20simple%20network%20architecture%2C%20the%20Transformer%2C%20based%20solely%20on%20attention%20mechanisms%2C%20dispensing%20with%20recurrence%20and%20convolutions%20entirely.%20Experiments%20on%20two%20machine%20translation%20tasks%20show%20these%20models%20to%20be%20superior%20in%20quality%20while%20being%20more%20parallelizable%20and%20requiring%20significantly%20less%20time%20to%20train.%20Our%20model%20achieves%2028.4%20BLEU%20on%20the%20WMT%202014%20English-to-German%20translation%20task%2C%20improving%20over%20the%20existing%20best%20results%2C%20including%20ensembles%20by%20over%202%20BLEU.%20On%20the%20WMT%202014%20English-to-French%20translation%20task%2C%20our%20model%20establishes%20a%20new%20single-model%20state-of-the-art%20BLEU%20score%20of%2041.8%20after%20training%20for%203.5%20days%20on%20eight%20GPUs%2C%20a%20small%20fraction%20of%20the%20training%20costs%20of%20the%20best%20models%20from%20the%20literature.%20We%20show%20that%20the%20Transformer%20generalizes%20well%20to%20other%20tasks%20by%20applying%20it%20successfully%20to%20English%20constituency%20parsing%20both%20with%20large%20and%20limited%20training%20data.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1706.03762&amp;rft.aufirst=Ashish&amp;rft.aulast=Vaswani&amp;rft.au=Ashish%20Vaswani&amp;rft.au=Noam%20Shazeer&amp;rft.au=Niki%20Parmar&amp;rft.au=Jakob%20Uszkoreit&amp;rft.au=Llion%20Jones&amp;rft.au=Aidan%20N.%20Gomez&amp;rft.au=Lukasz%20Kaiser&amp;rft.au=Illia%20Polosukhin&amp;rft.date=2017-12-05"></span>
  <div class="csl-entry">Vincent, Pascal, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,” n.d., 38.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stacked%20Denoising%20Autoencoders%3A%20Learning%20Useful%20Representations%20in%20a%20Deep%20Network%20with%20a%20Local%20Denoising%20Criterion&amp;rft.aufirst=Pascal&amp;rft.aulast=Vincent&amp;rft.au=Pascal%20Vincent&amp;rft.au=Hugo%20Larochelle&amp;rft.au=Isabelle%20Lajoie&amp;rft.au=Yoshua%20Bengio&amp;rft.au=Pierre-Antoine%20Manzagol&amp;rft.pages=38&amp;rft.language=en"></span>
  <div class="csl-entry">Zhang, Han, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. “Self-Attention Generative Adversarial Networks.” arXiv, June 14, 2019. <a href="https://doi.org/10.48550/arXiv.1805.08318">https://doi.org/10.48550/arXiv.1805.08318</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1805.08318&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Self-Attention%20Generative%20Adversarial%20Networks&amp;rft.description=In%20this%20paper%2C%20we%20propose%20the%20Self-Attention%20Generative%20Adversarial%20Network%20(SAGAN)%20which%20allows%20attention-driven%2C%20long-range%20dependency%20modeling%20for%20image%20generation%20tasks.%20Traditional%20convolutional%20GANs%20generate%20high-resolution%20details%20as%20a%20function%20of%20only%20spatially%20local%20points%20in%20lower-resolution%20feature%20maps.%20In%20SAGAN%2C%20details%20can%20be%20generated%20using%20cues%20from%20all%20feature%20locations.%20Moreover%2C%20the%20discriminator%20can%20check%20that%20highly%20detailed%20features%20in%20distant%20portions%20of%20the%20image%20are%20consistent%20with%20each%20other.%20Furthermore%2C%20recent%20work%20has%20shown%20that%20generator%20conditioning%20affects%20GAN%20performance.%20Leveraging%20this%20insight%2C%20we%20apply%20spectral%20normalization%20to%20the%20GAN%20generator%20and%20find%20that%20this%20improves%20training%20dynamics.%20The%20proposed%20SAGAN%20achieves%20the%20state-of-the-art%20results%2C%20boosting%20the%20best%20published%20Inception%20score%20from%2036.8%20to%2052.52%20and%20reducing%20Frechet%20Inception%20distance%20from%2027.62%20to%2018.65%20on%20the%20challenging%20ImageNet%20dataset.%20Visualization%20of%20the%20attention%20layers%20shows%20that%20the%20generator%20leverages%20neighborhoods%20that%20correspond%20to%20object%20shapes%20rather%20than%20local%20regions%20of%20fixed%20shape.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1805.08318&amp;rft.aufirst=Han&amp;rft.aulast=Zhang&amp;rft.au=Han%20Zhang&amp;rft.au=Ian%20Goodfellow&amp;rft.au=Dimitris%20Metaxas&amp;rft.au=Augustus%20Odena&amp;rft.date=2019-06-14"></span>
  <div class="csl-entry">Zhang, Richard, Phillip Isola, and Alexei A. Efros. “Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction.” arXiv, April 20, 2017. <a href="https://doi.org/10.48550/arXiv.1611.09842">https://doi.org/10.48550/arXiv.1611.09842</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1611.09842&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Split-Brain%20Autoencoders%3A%20Unsupervised%20Learning%20by%20Cross-Channel%20Prediction&amp;rft.description=We%20propose%20split-brain%20autoencoders%2C%20a%20straightforward%20modification%20of%20the%20traditional%20autoencoder%20architecture%2C%20for%20unsupervised%20representation%20learning.%20The%20method%20adds%20a%20split%20to%20the%20network%2C%20resulting%20in%20two%20disjoint%20sub-networks.%20Each%20sub-network%20is%20trained%20to%20perform%20a%20difficult%20task%20--%20predicting%20one%20subset%20of%20the%20data%20channels%20from%20another.%20Together%2C%20the%20sub-networks%20extract%20features%20from%20the%20entire%20input%20signal.%20By%20forcing%20the%20network%20to%20solve%20cross-channel%20prediction%20tasks%2C%20we%20induce%20a%20representation%20within%20the%20network%20which%20transfers%20well%20to%20other%2C%20unseen%20tasks.%20This%20method%20achieves%20state-of-the-art%20performance%20on%20several%20large-scale%20transfer%20learning%20benchmarks.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1611.09842&amp;rft.aufirst=Richard&amp;rft.aulast=Zhang&amp;rft.au=Richard%20Zhang&amp;rft.au=Phillip%20Isola&amp;rft.au=Alexei%20A.%20Efros&amp;rft.date=2017-04-20"></span>
  <div class="csl-entry">Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” arXiv, August 24, 2020. <a href="https://doi.org/10.48550/arXiv.1703.10593">https://doi.org/10.48550/arXiv.1703.10593</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.48550%2FarXiv.1703.10593&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Unpaired%20Image-to-Image%20Translation%20using%20Cycle-Consistent%20Adversarial%20Networks&amp;rft.description=Image-to-image%20translation%20is%20a%20class%20of%20vision%20and%20graphics%20problems%20where%20the%20goal%20is%20to%20learn%20the%20mapping%20between%20an%20input%20image%20and%20an%20output%20image%20using%20a%20training%20set%20of%20aligned%20image%20pairs.%20However%2C%20for%20many%20tasks%2C%20paired%20training%20data%20will%20not%20be%20available.%20We%20present%20an%20approach%20for%20learning%20to%20translate%20an%20image%20from%20a%20source%20domain%20%24X%24%20to%20a%20target%20domain%20%24Y%24%20in%20the%20absence%20of%20paired%20examples.%20Our%20goal%20is%20to%20learn%20a%20mapping%20%24G%3A%20X%20%5Crightarrow%20Y%24%20such%20that%20the%20distribution%20of%20images%20from%20%24G(X)%24%20is%20indistinguishable%20from%20the%20distribution%20%24Y%24%20using%20an%20adversarial%20loss.%20Because%20this%20mapping%20is%20highly%20under-constrained%2C%20we%20couple%20it%20with%20an%20inverse%20mapping%20%24F%3A%20Y%20%5Crightarrow%20X%24%20and%20introduce%20a%20cycle%20consistency%20loss%20to%20push%20%24F(G(X))%20%5Capprox%20X%24%20(and%20vice%20versa).%20Qualitative%20results%20are%20presented%20on%20several%20tasks%20where%20paired%20training%20data%20does%20not%20exist%2C%20including%20collection%20style%20transfer%2C%20object%20transfiguration%2C%20season%20transfer%2C%20photo%20enhancement%2C%20etc.%20Quantitative%20comparisons%20against%20several%20prior%20methods%20demonstrate%20the%20superiority%20of%20our%20approach.&amp;rft.identifier=urn%3Adoi%3A10.48550%2FarXiv.1703.10593&amp;rft.aufirst=Jun-Yan&amp;rft.aulast=Zhu&amp;rft.au=Jun-Yan%20Zhu&amp;rft.au=Taesung%20Park&amp;rft.au=Phillip%20Isola&amp;rft.au=Alexei%20A.%20Efros&amp;rft.date=2020-08-24"></span>
</div></body>
</html>
